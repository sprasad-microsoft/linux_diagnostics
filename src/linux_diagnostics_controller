# linux_diagnostics/src/controller/main.py

#!/usr/bin/env python3
# Linux Diagnostics Controller Daemon

import os
import subprocess
import yaml
import tarfile
import time
import logging
from datetime import datetime
from logging.handlers import SysLogHandler

CONTROLLER_VERSION = 1
CONFIG_FILE = "./config.yaml"  # Replace with the actual path to your YAML config file
OUTPUT_DIR = "/var/log/linux_diagnostics"  # Default output directory for diagnostics logs if not specified in the config
PIDFILE = "/var/run/linux_diagnostics/controller.pid"  # Default PID file location
DIAGNOSTICS_CMD = ["diagnostics_tool"]  # Replace with the actual diagnostics command

# Configure logging to syslog
logger = logging.getLogger("linux_diagnostics")
logger.setLevel(logging.INFO)
syslog_handler = SysLogHandler(address="/dev/log")
formatter = logging.Formatter('%(name)s: %(levelname)s %(message)s')
syslog_handler.setFormatter(formatter)
logger.addHandler(syslog_handler)

# Check the mounted network filesystems
filesystem_types = []
try:
    cifs_output = subprocess.check_output(["mount", "-t", "cifs"], stderr=subprocess.DEVNULL, text=True)
    if cifs_output.strip():
        filesystem_types.append("cifs")
except subprocess.CalledProcessError:
    logger.info("No CIFS filesystems mounted.")

try:
    nfs_output = subprocess.check_output(["mount", "-t", "nfs"], stderr=subprocess.DEVNULL, text=True)
    if nfs_output.strip():
        filesystem_types.append("nfs")
except subprocess.CalledProcessError:
    logger.info("No NFS filesystems mounted.")

def read_config(config_file):
    """Reads the YAML configuration file."""
    with open(config_file, 'r') as file:
        return yaml.safe_load(file)

def run_monitors(monitors):
    """Runs the monitors in the background."""
    processes = {}
    for exe in monitors:
        try:
            process = subprocess.Popen(exe, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            processes[exe] = process
            logger.info(f"Started process {exe}")
        except Exception as e:
            logger.error(f"Failed to start {exe}: {e}")
            return None
    logger.info("All monitors started successfully.")
    return processes

def monitor_processes(processes):
    """Monitors the processes for anomalies."""
    for exe, process in processes.items():
        retcode = process.poll()
        if retcode is not None:  # Process has exited
            logger.warning(f"Process {exe} exited with code {retcode}")
            return exe
    return None

def run_diagnostics(output_dir):
    """Runs the diagnostics tool and saves the output to a tarball."""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    tarball_path = os.path.join(output_dir, f"diagnostics_{timestamp}.tar.gz")
    diagnostics_dir = os.path.join(output_dir, f"diagnostics_{timestamp}")

    try:
        os.makedirs(diagnostics_dir, exist_ok=True)
        DIAGNOSTICS_CMD.extend(["-d", diagnostics_dir])
        subprocess.run(DIAGNOSTICS_CMD, stderr=subprocess.STDOUT, check=True)
        
        with tarfile.open(tarball_path, "w:gz") as tar:
            tar.add(diagnostics_dir, arcname=os.path.basename(diagnostics_dir))
        
        logger.info(f"Diagnostics saved to {tarball_path}")
    except Exception as e:
        logger.error(f"Failed to run diagnostics: {e}")

def manage_tarballs(output_dir, controller_config):
    """Manages tarballs by checking disk usage and count limits."""
    max_disk_usage = controller_config.get("max_disk_usage", (1024 * 1024 * 1024))  # Default: 10GB
    max_tarball_count = controller_config.get("max_tarball_count", 10)  # Default: 10 tarballs

    # Get list of tarballs sorted by creation time
    tarballs = sorted(
        [os.path.join(output_dir, f) for f in os.listdir(output_dir) if f.endswith(".tar.gz")],
        key=os.path.getctime
    )

    # Calculate total disk usage
    total_size = sum(os.path.getsize(tarball) for tarball in tarballs)

    # Remove older tarballs if limits are exceeded
    while total_size > max_disk_usage or len(tarballs) > max_tarball_count or get_disk_space_percentage(output_dir) < 10:
        oldest_tarball = tarballs.pop(0)
        try:
            total_size -= os.path.getsize(oldest_tarball)
            os.remove(oldest_tarball)
            logger.info(f"Deleted old tarball {oldest_tarball} to free up space")
        except Exception as e:
            logger.error(f"Failed to delete tarball {oldest_tarball}: {e}")

def get_disk_space_percentage(directory):
    """Returns the available disk space percentage at the given directory."""
    statvfs = os.statvfs(directory)
    available_space = statvfs.f_bavail * statvfs.f_frsize
    total_space = statvfs.f_blocks * statvfs.f_frsize
    return (available_space / total_space) * 100

def configure_cgroup_limits(cpu_limit, memory_limit):
    """Configures a cgroup with specified CPU and memory limits."""
    if cpu_limit or memory_limit:
        cgroup_name = "linux_diagnostics"
        cgroup_path = f"/sys/fs/cgroup/{cgroup_name}"

        # Create a new cgroup
        try:
            os.makedirs(cgroup_path, exist_ok=True)
            logger.info(f"Created cgroup {cgroup_name}")

            # Apply CPU limit if specified
            if cpu_limit:
                cpu_quota_path = os.path.join(cgroup_path, "cpu.max")
                with open(cpu_quota_path, "w") as f:
                    f.write(f"{int(cpu_limit * 100000)} 100000")
                logger.info(f"Set CPU limit to {cpu_limit} cores")

            # Apply memory limit if specified
            if memory_limit:
                memory_limit_path = os.path.join(cgroup_path, "memory.max")
                with open(memory_limit_path, "w") as f:
                    f.write(f"{int(memory_limit * 1024 * 1024)}")
                logger.info(f"Set memory limit to {memory_limit} bytes")

            # Add the current process to the cgroup
            tasks_path = os.path.join(cgroup_path, "cgroup.procs")
            with open(tasks_path, "w") as f:
                f.write(str(os.getpid()))
            logger.info(f"Added process {os.getpid()} to cgroup {cgroup_name}")
        except Exception as e:
            logger.error(f"Failed to configure cgroup: {e}")
            return
        
def get_monitors(anomalies):
    """Determines the monitors to run based on anomalies."""
    monitors = []
    if "cifs" in filesystem_types:
        if "server_latency" in anomalies:
            monitors.append("smbslower")
        if "vfs_callback_latency" in anomalies:
            monitors.append("smbvfsslower")
        if "server_errors" in anomalies:
            monitors.append("smbiosnoop")
        if "vfs_callback_errors" in anomalies:
            monitors.append("smbvfsiosnoop")
    if "nfs" in filesystem_types:
        if "server_latency" in anomalies:
            monitors.append("nfsslower")
        if "vfs_callback_latency" in anomalies:
            monitors.append("nfsvfsslower")
        if "server_errors" in anomalies:
            monitors.append("nfsiosnoop")
        if "vfs_callback_errors" in anomalies:
            monitors.append("nfsvfsiosnoop")

    if not monitors:
        logger.error("No monitors to run. Exiting.")
        return None
    logger.info(f"Monitors to run: {monitors}")
    return monitors

def main():
    """Main function for the controller daemon."""
    # Check if the script is running as root
    if os.geteuid() != 0:
        logger.error("Controller daemon must be run as root.")
        return
    
    config = read_config(CONFIG_FILE)
    controller_config = config.get("controller_config", {})
    if not config or not controller_config:
        logger.error("Failed to read configuration file.")
        return

    # First check if the controller version matches
    if CONTROLLER_VERSION != controller_config.get("version", 0):
        logger.error(f"Controller version mismatch. Expected {CONTROLLER_VERSION}, got {controller_config.get('version', 0)}.")
        return
    
    # Now create the output directory and PID file
    output_dir = controller_config.get("output_directory", OUTPUT_DIR)
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        logger.info(f"Created output directory {output_dir}")

    pidfile = controller_config.get("pidfile", PIDFILE)
    pidfile_dir = os.path.dirname(pidfile)
    if not os.path.exists(pidfile_dir):
        os.makedirs(pidfile_dir)
        logger.info(f"Created PID file directory {pidfile_dir}")

    # Check if the PID file exists
    if os.path.exists(pidfile):
        try:
            with open(pidfile, "r") as f:
                existing_pid = int(f.read().strip())
            # Check if the process with the PID is still running
            if os.path.exists(f"/proc/{existing_pid}"):
                logger.error(f"Another instance of the controller is already running with PID {existing_pid}. Exiting.")
                return
        except Exception as e:
            logger.warning(f"Failed to read or validate PID file: {e}. Proceeding to create a new PID file.")

    # Create or overwrite the PID file with the current process ID
    try:
        with open(pidfile, "w") as f:
            f.write(str(os.getpid()))
        logger.info(f"Created PID file at {pidfile} with PID {os.getpid()}")
    except Exception as e:
        logger.error(f"Failed to create PID file: {e}")
        return

    # Check for CPU and memory constraints in the configuration
    cpu_limit = controller_config.get("cpu_limit", 0)
    memory_limit = controller_config.get("memory_limit", 0)
    configure_cgroup_limits(cpu_limit, memory_limit)
    
    anomalies = controller_config.get("anomalies", {})
    if not anomalies:
        logger.error("No anomalies specified in the configuration. Exiting.")
        return

    monitors = get_monitors(anomalies)
    if not monitors:
        logger.error("No monitors to run based on the specified anomalies. Exiting.")
        return

    try:
        while True:
            logger.info("Running monitors...")
            processes = run_monitors(monitors)
            if not processes:
                logger.error("Failed to start monitors. Exiting.")
                return
            time.sleep(1)  # Polling interval
            anomaly_exe = monitor_processes(processes)
            if anomaly_exe:
                logger.warning(f"Anomaly detected in {anomaly_exe}. Running diagnostics...")
                run_diagnostics(output_dir)
                manage_tarballs(output_dir, controller_config)
                logger.info("Restarting monitors...")
    except KeyboardInterrupt:
        logger.info("Shutting down daemon...")
    finally:
        if processes:
            logger.info("Terminating all running monitors...")
            for process in processes.values():
                process.terminate()
                logger.info(f"Terminated process {process.pid}")

if __name__ == "__main__":
    main()